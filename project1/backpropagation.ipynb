{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alina Artemiuk\n",
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l=[2, 3, 2]):\n",
    "        super().__init__()\n",
    "\n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l) - 1\n",
    "        self.n_l = n_l\n",
    "\n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function\n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i: None for i in range(1, self.L + 1)}\n",
    "        self.a = {i: None for i in range(self.L + 1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i: None for i in range(1, self.L + 1)}\n",
    "        self.dL_db = {i: None for i in range(1, self.L + 1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i: lambda x: torch.tanh(x) for i in range(1, self.L + 1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {\n",
    "            i: lambda x: (1 / (torch.cosh(x) ** 2))\n",
    "            for i in range(1, self.L + 1)\n",
    "        }\n",
    "\n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to\n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L + 1)})\n",
    "        for i in range(1, self.L + 1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i - 1], out_features=n_l[i])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "\n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L + 1):\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i - 1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    # eq. 5\n",
    "    dL_da_L = - 2 * torch.sub(y_true, y_pred)\n",
    "    da_L_dz_L = model.df[model.L](model.z[model.L])  # da/dz=f'(z) for the first backpropagation layer\n",
    "\n",
    "    # eq. 4\n",
    "    delta_L = torch.einsum('ij, ij -> ij', dL_da_L, da_L_dz_L)\n",
    "    dL_dw_L = torch.einsum('ij, ik -> jk', delta_L, model.a[model.L - 1])\n",
    "\n",
    "    model.dL_dw[model.L] = dL_dw_L\n",
    "    model.dL_db[model.L] = torch.squeeze(delta_L)\n",
    "    delta_prev = delta_L\n",
    "\n",
    "    for l in range(model.L - 1, 0, -1):\n",
    "        # eq. 6\n",
    "        df_z = model.df[l](model.z[l])  # f'(z)\n",
    "        w_prev = model.fc[str(l + 1)].weight.data\n",
    "        delta_curr = torch.einsum('ij, jk, ik -> ik', delta_prev, w_prev, df_z)\n",
    "        dL_dw_L = torch.einsum('ij, ik -> jk', delta_curr, model.a[l - 1])\n",
    "\n",
    "        model.dL_dw[l] = dL_dw_L\n",
    "        model.dL_db[l] = torch.squeeze(delta_curr)\n",
    "        delta_prev = delta_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-8.5505e-09, -9.9667e-09],\n",
      "        [ 2.5254e-10,  2.9436e-10],\n",
      "        [-1.9100e-04, -2.2263e-04]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[-8.5505e-09, -9.9667e-09],\n",
      "        [ 2.5254e-10,  2.9436e-10],\n",
      "        [-1.9100e-04, -2.2263e-04]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-9.7510e-10,  2.8799e-11, -2.1782e-05], grad_fn=<SqueezeBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([-9.7510e-10,  2.8799e-11, -2.1782e-05])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 2 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-2.1537e-09, -2.5104e-09],\n",
      "        [ 5.5454e-11,  6.4639e-11],\n",
      "        [-4.6432e-05, -5.4122e-05]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[-2.1537e-09, -2.5104e-09],\n",
      "        [ 5.5454e-11,  6.4639e-11],\n",
      "        [-4.6432e-05, -5.4122e-05]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-2.4560e-10,  6.3240e-12, -5.2951e-06], grad_fn=<SqueezeBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([-2.4560e-10,  6.3240e-12, -5.2951e-06])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 3 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-8.2948e-10, -9.6687e-10],\n",
      "        [ 2.2153e-11,  2.5823e-11],\n",
      "        [-1.8118e-05, -2.1119e-05]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[-8.2948e-10, -9.6687e-10],\n",
      "        [ 2.2153e-11,  2.5823e-11],\n",
      "        [-1.8118e-05, -2.1119e-05]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-9.4594e-11,  2.5264e-12, -2.0662e-06], grad_fn=<SqueezeBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([-9.4594e-11,  2.5264e-12, -2.0662e-06])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 4 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-6.2492e-10, -7.2843e-10],\n",
      "        [ 1.9784e-11,  2.3060e-11],\n",
      "        [-1.3480e-05, -1.5713e-05]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[-6.2492e-10, -7.2843e-10],\n",
      "        [ 1.9784e-11,  2.3060e-11],\n",
      "        [-1.3480e-05, -1.5713e-05]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-7.1267e-11,  2.2561e-12, -1.5373e-06], grad_fn=<SqueezeBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([-7.1267e-11,  2.2561e-12, -1.5373e-06])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 5 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-4.4828e-10, -5.2253e-10],\n",
      "        [ 1.5838e-11,  1.8461e-11],\n",
      "        [-9.6575e-06, -1.1257e-05]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[-4.4828e-10, -5.2253e-10],\n",
      "        [ 1.5838e-11,  1.8461e-11],\n",
      "        [-9.6575e-06, -1.1257e-05]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-5.1122e-11,  1.8062e-12, -1.1013e-06], grad_fn=<SqueezeBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([-5.1122e-11,  1.8062e-12, -1.1013e-06])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "tensor([[ 0.1498,  1.0011],\n",
      "        [-0.8157, -0.5564],\n",
      "        [ 0.5891,  0.1175]])\n",
      "tensor([-0.1348, -0.3174,  0.0592])\n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "main_test(backpropagation, model, verbose=True, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alina Artemiuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[ -3.0457e-11,  -3.0457e-11,  -3.0457e-11,  ...,  -3.0457e-11,\n",
      "          -3.0457e-11,  -3.0457e-11],\n",
      "        [  1.0634e-31,   1.0634e-31,   1.0634e-31,  ...,   1.0634e-31,\n",
      "           1.0634e-31,   1.0634e-31],\n",
      "        [  2.9139e-11,   2.9139e-11,   2.9139e-11,  ...,   2.9139e-11,\n",
      "           2.9139e-11,   2.9139e-11],\n",
      "        ...,\n",
      "        [ -8.5970e-06,  -8.5970e-06,  -8.5970e-06,  ...,  -8.5970e-06,\n",
      "          -8.5970e-06,  -8.5970e-06],\n",
      "        [-2.1297e-114, -2.1297e-114, -2.1297e-114,  ..., -2.1297e-114,\n",
      "         -2.1297e-114, -2.1297e-114],\n",
      "        [ -1.4547e-72,  -1.4547e-72,  -1.4547e-72,  ...,  -1.4547e-72,\n",
      "          -1.4547e-72,  -1.4547e-72]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[-3.0457e-11, -3.0457e-11, -3.0457e-11,  ..., -3.0457e-11,\n",
      "         -3.0457e-11, -3.0457e-11],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 2.9139e-11,  2.9139e-11,  2.9139e-11,  ...,  2.9139e-11,\n",
      "          2.9139e-11,  2.9139e-11],\n",
      "        ...,\n",
      "        [-8.5970e-06, -8.5970e-06, -8.5970e-06,  ..., -8.5970e-06,\n",
      "         -8.5970e-06, -8.5970e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([  7.1828e-11,  -2.5079e-31,  -6.8719e-11,  -2.0209e-39,   1.6396e-12,\n",
      "          6.0320e-03, -4.2105e-118,  -1.6173e-06,  -5.4178e-64,   1.1343e-79,\n",
      "         -9.2039e-22,   2.8670e-36,  -8.1549e-44,   2.0275e-05,  5.0226e-114,\n",
      "          3.4308e-72], grad_fn=<SqueezeBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([ 7.1828e-11,  0.0000e+00, -6.8719e-11,  0.0000e+00,  1.6396e-12,\n",
      "         6.0320e-03,  0.0000e+00, -1.6173e-06,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0275e-05,  0.0000e+00,\n",
      "         0.0000e+00])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "tensor([[-0.0555, -0.0465, -0.0433,  ..., -0.0346, -0.0224, -0.0017],\n",
      "        [ 0.0827,  0.0829,  0.1257,  ...,  0.1251,  0.1222,  0.0656],\n",
      "        [-0.0684, -0.0541, -0.0055,  ..., -0.0258, -0.0812, -0.0249],\n",
      "        ...,\n",
      "        [ 0.0007, -0.0212,  0.0453,  ..., -0.0113,  0.0076,  0.0188],\n",
      "        [ 0.3883,  0.3835,  0.4080,  ...,  0.3544,  0.3788,  0.4239],\n",
      "        [ 0.2792,  0.2846,  0.2635,  ...,  0.2878,  0.2551,  0.2643]])\n",
      "tensor([ 0.0753, -0.1991,  0.1236,  0.3044, -0.0855, -0.0312, -0.9829,  0.0790,\n",
      "         0.4745,  0.6082,  0.2104, -0.3203, -0.3292, -0.0245, -0.9294, -0.5529])\n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([24 * 24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=True, data='mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('nglm-env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ae36e8c2cbd9e14d80419493f2540eab6c211be174ac39ce04705a74740d0d8b"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
